---
layout : post
title: 심층 신경망 훈련
category: ML
tag: Pragmatic
---

## Weigth 초기화
### Xavier / Glorot
 > 출력층의 분산과 입력층의 분산을 같게 한다.
 
Var(W)=2nin+nout

## Reference
 > 출력층의 분산이 입력층의 분산보다 커지는 현상

---
layout : post
title: 심층 신경망 훈련
category: ML
tag: Pragmatic
---

## Gradient 소실과 폭주
### Weigth 초기화
#### Xavier / Glorot
 > 출력층과 입력층의 gradient 분산을 같게 한다.

* sigmoid 일때
  * avg = 0, std = 인 정규 분포
  * -r ~ +r 사이의 균등 분포

$$
P\left( X=x \right) =1
$$ 

#### He
* ReLU와 그 변종 함수들을 위한 초기화 전략

### Activation 함수
#### Dying ReLU
  * 훈련 중 일부 뉴런이 0이외의 값을 출력하지 않을 때 (큰 학습률을 사용하면 뉴런 절반이 죽기도)
    * 가중치 합이 0이 되면 이후 0을 출력.
    
#### 대안        
* LeakyReLU
* RReLU
  * 훈련시에는 주어진 범위내의 a를 무작위로 선택하고, 테스트시에는 이의 평균을 사용
  * 규제의 역할도 수행하여 Overfitting 방지
* PReLU
  * a도 학습 파라미터로 정의
  * 대규모 데이터 셋에선 잘 동작하지만, 소규모에선 Overfitting 위험
* ELU
  * 훈련 시간이 줄고, 성능도 개선
  * 지수함수를 사용해서 계산이 느림
  
#### Summary
* ELU > LeakyReLU > ReLU > tanh > sigmoid
* 실행 속도가 중요하다면, LeakyReLU

### BatchNormalization
> 훈련중에도 weight를 정규화.

## Reference
 > 출력층의 분산이 입력층의 분산보다 커지는 현상

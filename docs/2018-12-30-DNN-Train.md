---
layout : post
title: 심층 신경망 훈련
category: ML
tag: Pragmatic
---

## Weigth 초기화
### Xavier / Glorot
 > 출력층과 입력층의 gradient 분산을 같게 한다.
  
$$
P\left( X=x \right) =1
$$ 

#### 정규 분포 (sigmoid)
Sigma = root(2/(ninput + noutput))

#### 균등 분포 (sigmoid)

## Reference
 > 출력층의 분산이 입력층의 분산보다 커지는 현상
